{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zeg4rjDd55vC"
      },
      "id": "Zeg4rjDd55vC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "XzrNvR4LO5e0",
      "metadata": {
        "id": "XzrNvR4LO5e0"
      },
      "source": [
        "# **Survey Data Cleaning**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ofdp6boM2AVK",
      "metadata": {
        "id": "ofdp6boM2AVK"
      },
      "source": [
        "**To start:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Download chrome extension or chrome ap: Open in Colab (program that allows you to write and execute Python in your browser it has nothing to do with the CBGLCollab). Only need to do this once.\n",
        "\n",
        "Download data from qualtrics: File must be in csv not excel and \"use numeric values.\" Don't modify or delete any rows.\n",
        "\n",
        "If necessary, re-code any programs, institutions, or questions in qualtrics to match code book.\n",
        "\n",
        "Press the first \"play\" button below. Allow colab to access your drive when prompted and then select \"choose files.\" Open the desired file.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3823b79a-bd81-425f-9328-07c976eedd08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "3823b79a-bd81-425f-9328-07c976eedd08",
        "outputId": "662659f2-41f7-4be7-8aa4-ee31e2578881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd264dcf-c709-4b60-a2b9-3196d29a4375\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dd264dcf-c709-4b60-a2b9-3196d29a4375\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Global Engagement Survey_Fall 2023 -Summer 2024_January 19, 2024_11.49.csv to Global Engagement Survey_Fall 2023 -Summer 2024_January 19, 2024_11.49.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import natsort as ns\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import io\n",
        "drive.mount('drive')\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3a767e-057b-4b89-a7dc-c2d981ecf007",
      "metadata": {
        "id": "bd3a767e-057b-4b89-a7dc-c2d981ecf007"
      },
      "source": [
        "5.   In the box below enter the:*actions for 22-23 cycle\n",
        "\n",
        "*a) file name exactly as written (including spaces) btw single quotations w .csv at the end (ie: '2021-2022 GES.csv')\n",
        "\n",
        "b) Redundant_program_cols (this is for insitutions that have 2 questions for program code ie term number and then the program. For 23-24 cycle this is CFHI and WPI). If need to add in the future, add name of the column that you want to delete). Don't need to do anything for 23-24 cycle.\n",
        "\n",
        "c) multi_answer_qs (name of the column as qualtrics geneates it, number of possible answer choices). If needed can adjust in the future. Don't need to do anything for 23-24 cycle.  \n",
        "\n",
        "*d) starting unique id number into the appropriate variables below.\n",
        "\n",
        "e) open_questions_pre/post: the name of the columns with any qualitative questions (e.g. 'Q14'). To update, either delete old column names or add new ones in quotes, separating each column with a comma. This includes any qualitative institution-specific questions. Don't need to do anything for the 23-24 cycle.\n",
        "\n",
        "f) useful_metadata: the name of any columns containing metadata that we want to include in the final files (e.g. name, intitution, program, etc.). The formatting for this is the same as the open_questions_pre/post. Don't need to do anything for the 22-23 cycle.\n",
        "\n",
        "g) merge_cols: The name of the institution, program, first name, middle name, and last name columns. This shouldn't need to be changed, it is just a work-around to another error.\n",
        "\n",
        "*6.   Scroll to the bottom of the code (#export files to drive) and enter the google drive folder where you want your cleaned data to go.\n",
        "\n",
        "*7.   Check the \"GES data cleaning code documentation\" to see if anything else needs to be updated. Don't need to do anything for the 23-24 cycle.\n",
        "\n",
        "*8.   Press the second \"play\" button to run the rest of the code, the cleaned files should appear in the desired folder after about a minute\n",
        "\n",
        "*9. Cleaned filed will appear in the google drive folder that you specified.\n",
        "Note: The code is generating a warning about a piece of the code we are no longer using. It is not an error. The code is functioning correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bef7b9b-bfbc-4284-a48e-05b483e10b0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bef7b9b-bfbc-4284-a48e-05b483e10b0d",
        "outputId": "99e39cf2-32f4-4bd1-e5c2-a01bc8ac90de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9577454fa256>:52: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df1.loc[:,'Pre_Post']=df1['Pre_Post'].astype(np.int32)\n",
            "<ipython-input-6-9577454fa256>:53: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df1.loc[:,'Program']=df1['Program'].astype(np.int32)\n",
            "<ipython-input-6-9577454fa256>:162: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
            "  df_full = pd.merge(df_pre_cleaned.loc[:,'Q1_1':],df_post_cleaned.loc[:,'Q1_1':],on=['UniqueID'],how='outer')\n",
            "<ipython-input-6-9577454fa256>:202: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_qual.loc[:,'Pre_Post']=df_qual.loc[:,'Pre_Post_x'].fillna(0)+df_qual.loc[:,'Pre_Post_y'].fillna(0)\n"
          ]
        }
      ],
      "source": [
        "from types import prepare_class\n",
        "\n",
        "#1:User inputted variables\n",
        "file_name = 'Global Engagement Survey_Fall 2023 -Summer 2024_January 19, 2024_11.49.csv'\n",
        "redundant_program_cols = ['CFHI','WPI']\n",
        "multi_answer_qs = [ ['PF42a',7],['PF43',10]]\n",
        "id_start = 1\n",
        "open_questions_pre = ['Q14', 'Q15', 'Q17', 'Q18', 'Q27', 'Q36', 'Q45', 'Q46', 'Q54', 'Q55', 'Q64', 'Q71', 'Q82', 'Q84_4_TEXT', 'Q85_TEXT', 'Q86_2_TEXT', 'Q87_2_TEXT', 'Q98_11_12_TEXT', 'CorSIP1', 'CornSIP2', 'ElWAD1', 'ElWAD2', 'ELWAD3', 'ElWAD4', 'ElWAD5', 'ElH1', 'ElH2', 'ElH3', 'WPI1', 'WPI1a']\n",
        "open_questions_post = [ 'Q14P', 'Q15P', 'Q17P', 'Q18P', 'Q27P', 'Q36P', 'Q45P', 'Q46P', 'Q54P', 'Q55P', 'Q64P', 'Q71P', 'Q82P', 'Q137P', 'Q139P', 'Q141P', 'Q143P', 'Q149P', 'Q151P', 'Q153P', 'PF33_10_TEXT', 'PF34_3_TEXT', 'PF35_6_TEXT', 'PF36_7_TEXT', 'PF41_1_TEXT','ELWAD1P', 'ElWAD2P', 'ELWAD3P', 'ELWAD4P', 'ElH1P', 'ElH2P', 'ElH3P', 'ElH4P', 'ElH5P', 'WPI1P', 'WPI1aP', 'WPI2P', 'WPI3P', 'WPI4P', 'Hav1P', 'Have2P', 'NEU1P', 'NEU2P', 'NEU3P']\n",
        "useful_metadata = ['Q1_1','Q1_2','Q1_3','UniqueID', 'Institution','Program','Pre_Post_x','Pre_Post_y']\n",
        "merge_cols = ['Institution', 'Program', 'Q1_1', 'Q1_2', 'Q1_3']\n",
        "#read file into dataframe\n",
        "df1 = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "question_text = df1.iloc[:1,:]\n",
        "question_text = question_text[open_questions_pre + open_questions_post]\n",
        "df1 = df1.iloc[2:,:]\n",
        "\n",
        "#2:Initial formatting changes:\n",
        "\n",
        "#fixing the institution column name - Qualtrics generates it with extra characters\n",
        "df1 = df1.rename(\n",
        "    columns={\n",
        "        'Institution\\xa0':'Institution',\n",
        "      })\n",
        "\n",
        "#Combine all program columns into one\n",
        "#first delete redundant columns (e.g. the place-based/virtual/other question for CFHI)\n",
        "df1 = df1.drop(df1.loc[:,redundant_program_cols],axis = 1)\n",
        "\n",
        "#Then combine remaining columns -- take all cols betweeen \"institution\" and \"email\" and merge them, each row should only have a value in one column\n",
        "program_start = df1.columns.get_loc('Institution')+1\n",
        "program_end = df1.columns.get_loc('Email')\n",
        "df1 = df1.rename(\n",
        "    columns = {\n",
        "        df1.columns[program_start]: \"Program\",\n",
        "    }\n",
        ")\n",
        "for i in range(program_start,program_end):\n",
        "  if df1.columns[i] not in redundant_program_cols:\n",
        "    df1['Program'] = df1['Program'].combine_first(df1.iloc[:,i])\n",
        "df1 = df1.drop(df1.iloc[:,program_start+1:program_end],axis=1)\n",
        "\n",
        "#Formatting emails and middle initial/name -- make emails lowercase and trim whitespace to serve as a unique identifier, removes middle initials entered as \"None\" or \"N/A\"\n",
        "df1['Email'] = df1['Email'].str.lower()\n",
        "df1['Email'] = df1['Email'].str.strip()\n",
        "df1['Q1_2'] = df1['Q1_2'].str.title().str.strip()\n",
        "df1.loc[(df1['Q1_2']=='None')|(df1['Q1_2']=='N/A'),'Q1_2'] = pd.Series(dtype=str)\n",
        "\n",
        "#ensure pre_post column stored as numbers rather than text\n",
        "df1 = df1[pd.notna(df1['Program']) & pd.notna(df1['Pre_Post'])]\n",
        "df1.loc[:,'Pre_Post']=df1['Pre_Post'].astype(np.int32)\n",
        "df1.loc[:,'Program']=df1['Program'].astype(np.int32)\n",
        "\n",
        "#remove rows without values in Q6\n",
        "df1 = df1[pd.notna(df1['Q6'])|pd.notna(df1['Q6P'])]\n",
        "\n",
        "#3:Drop duplicates\n",
        "\n",
        "#sort by progress and duration\n",
        "df = df1.sort_values(by=['Progress','Duration (in seconds)'],axis=0,ascending=[False,False])\n",
        "#delete unneeded metadata columns\n",
        "df = df.drop(['Status','IPAddress'],axis=1).drop(df.loc[:,'Duration (in seconds)':'UserLanguage'],axis=1)\n",
        "#drop duplicates -- because of the sorting, we are able to simply keep the first record of any duplicates\n",
        "df = df.drop_duplicates(subset=['Email','Program','Pre_Post'],keep='first')\n",
        "\n",
        "\n",
        "#4:Unique IDs\n",
        "#create UniqueID column\n",
        "df.insert(df.columns.get_loc('Q1_3')+1,'UniqueID',\"\")\n",
        "\n",
        "#Generate unique ids for pre\n",
        "unique_ids = range(id_start,id_start+len(df[df['Pre_Post']<2]))\n",
        "df.loc[df['Pre_Post']<2,'UniqueID']=unique_ids\n",
        "\n",
        "#Generate unique ids for post\n",
        "\n",
        "#create subset of the pre data containing just emails, program # and UniqueIDs\n",
        "emails_pre = df.loc[df['Pre_Post']<2,['Email','Program','UniqueID']]\n",
        "emails_pre = emails_pre.rename(\n",
        "    columns ={\n",
        "        'UniqueID':'id'\n",
        "    })\n",
        "\n",
        "#split pre and post data\n",
        "df_pre = df[df['Pre_Post']<2]\n",
        "df_post1 = df[df['Pre_Post']>1]\n",
        "\n",
        "#Assigns unique ids for post-survey matched cases\n",
        "df_post = pd.merge(df_post1, emails_pre, on =['Email','Program'], how ='left')\n",
        "\n",
        "df_post['UniqueID']=df_post['id']\n",
        "#removes redundant column\n",
        "df_post = df_post.drop('id',axis=1)\n",
        "#generate ids for unmatched post starting at 1+the max of the unique ids in the pre-survey\n",
        "post_ids = range(int(max(df_pre['UniqueID']))+1,len(df_post[pd.isna(df_post['UniqueID'])])+int(max(df_pre['UniqueID']))+1)\n",
        "df_post.loc[pd.isna(df_post['UniqueID']),'UniqueID']=post_ids\n",
        "\n",
        "#5:Begin building output files\n",
        "\n",
        "#create researcher file\n",
        "researcher = df_post[['UniqueID','Q1_1','Q1_2','Q1_3', 'Institution', 'Program', 'Res1','Res2']]\n",
        "\n",
        "#remove post questions from pre-survey and pre questions from post-survey\n",
        "df_pre_cleaned = df_pre.drop(df_pre.loc[:,'Q6P':],axis=1,inplace=False)\n",
        "index1 = df_post.columns.get_loc('Q6P')\n",
        "index2 = df_post.columns.get_loc('Q6')\n",
        "df_post_cleaned = df_post.drop(df_post.iloc[:,index2:index1],axis=1,inplace=False).drop(df_post.loc[:,['Res1','Res2']],axis=1,inplace=False).drop(df_post.loc[:,'CAP':],axis=1,inplace=False)\n",
        "\n",
        "#6:Adjust demographic and program factor data\n",
        "\n",
        "#For any students selecting multiple racial categories, re-assigns column to \"8\" for \"other\"\n",
        "df_pre_cleaned.loc[df_pre_cleaned['Q85'].str.len()>1,'Q85']=8\n",
        "\n",
        "\n",
        "\n",
        "#Split any columns with multiple possible answers into separate columns for each choice -- 1 for yes, empty for no\n",
        "for array in multi_answer_qs:\n",
        "  cols_add = []\n",
        "  col_loc = df_post_cleaned.columns.get_loc(array[0])\n",
        "  for i in range(array[1],0,-1):\n",
        "    col_name = array[0] + '_' + str(i)\n",
        "    df_post_cleaned.insert(col_loc,col_name,\"\")\n",
        "    cols_add.append([i,col_name])\n",
        "    df_post_cleaned.loc[df_post_cleaned[array[0]].str.find(str(i))>-1,col_name]=1\n",
        "  df_post_cleaned = df_post_cleaned.drop(array[0],axis=1)\n",
        "\n",
        "#7:Generate final outputs\n",
        "\n",
        "\n",
        "# Split into quant and qual based on user-inputted lists at top\n",
        "\"\"\"\n",
        "pre_open_q = []\n",
        "q_names_pre = []\n",
        "for array in open_questions_pre:\n",
        "  pre_open_q.append(array[0])\n",
        "  q_names_pre.append(array[1])\n",
        "post_open_q = []\n",
        "q_names_post = []\n",
        "for array in open_questions_post:\n",
        "  post_open_q.append(array[0])\n",
        "  q_names_post.append(array[1])\n",
        "\"\"\"\n",
        "\n",
        "pre_quant = df_pre_cleaned.drop(open_questions_pre,axis=1)\n",
        "post_quant = df_post_cleaned.drop(open_questions_post,axis=1)\n",
        "\n",
        "\n",
        "#Create matched column for quant -- for the pre data, check each case to see if it exists in the post, assign value to \"Matched\" column accordingly, same for post.\n",
        "quant_dbs = [pre_quant,post_quant]\n",
        "for i in range(2):\n",
        "  quant_dbs[i].insert(quant_dbs[i].columns.get_loc('Program')+1,'Matched',\"\")\n",
        "  quant_dbs[i].loc[quant_dbs[i]['UniqueID'].isin(quant_dbs[(i+1)%2]['UniqueID']),'Matched'] =3\n",
        "  quant_dbs[i].loc[~(quant_dbs[i]['UniqueID'].isin(quant_dbs[(i+1)%2]['UniqueID'])),'Matched'] = i+1\n",
        "\n",
        "#9:Merging qual responses\n",
        "\n",
        "\n",
        "#recreate the full dataset (qual and quant, pre and post)\n",
        "\n",
        "\n",
        "df_full = pd.merge(df_pre_cleaned.loc[:,'Q1_1':],df_post_cleaned.loc[:,'Q1_1':],on=['UniqueID'],how='outer')\n",
        "\n",
        "for col in merge_cols:\n",
        "  df_full[col+'_x'] = df_full[col+'_x'].combine_first(df_full[col+'_y'])\n",
        "  df_full.rename(\n",
        "    columns = {\n",
        "        col+'_x' : col\n",
        "    }, inplace = True\n",
        "  )\n",
        "#Add the qual question text into the dataset\n",
        "df_qual_combined = pd.concat([question_text, df_full], ignore_index=True)\n",
        "#Filter full dataset to include only qual questions and the necessary metadata\n",
        "df_qual = df_qual_combined[useful_metadata + open_questions_pre + open_questions_post]\n",
        "\n",
        "#reorder qual columns\n",
        "\n",
        "#The idea here is to search the column names for certain letters that are indicative of different questions -- we can identify the program factors because they start with \"PF\", the demographic questions because they start with \"Q\" and end with \"TEXT\", and the institution specific questions as the only non-metadata columns that don't start with \"Q\" or \"PF\".\n",
        "qual_cols = df_qual.columns[8:]\n",
        "qual_qs=[]\n",
        "demog_qs = []\n",
        "qual_program_factors = []\n",
        "qual_info = []\n",
        "inst_spec_qual = []\n",
        "for col in qual_cols:\n",
        "  if re.search('^Q\\d',col) != None:\n",
        "    if re.search('TEXT$',col) != None:\n",
        "      demog_qs.append(col)\n",
        "    else:\n",
        "      qual_qs.append(col)\n",
        "  elif re.search('^PF',col) != None:\n",
        "    qual_program_factors.append(col)\n",
        "  else:\n",
        "    inst_spec_qual.append(col)\n",
        "#Take the separate sets of column names and sort them to put, for example, \"Q14P\" directly after \"Q14\". The ns.natsorted function accomplishes this, since a standard sort would place \"Q120\" before \"Q15\".\n",
        "sorted_qs = ns.natsorted(qual_qs)\n",
        "sorted_pf = ns.natsorted(qual_program_factors)\n",
        "sorted_is = ns.natsorted(inst_spec_qual,alg=ns.IGNORECASE)\n",
        "sorted_ds = ns.natsorted(demog_qs)\n",
        "\n",
        "#Generate a new Pre_Post column to reflect the \"matched\" quant column (1 for pre only, 2 for post only, 3 for both)\n",
        "df_qual.loc[:,'Pre_Post']=df_qual.loc[:,'Pre_Post_x'].fillna(0)+df_qual.loc[:,'Pre_Post_y'].fillna(0)\n",
        "df_qual_final = df_qual[['Q1_1','Q1_2','Q1_3','UniqueID','Institution','Program','Pre_Post'] +  sorted_qs + sorted_ds + sorted_pf + sorted_is]\n",
        "\n",
        "#export files to drive -- edit text after \"drive/My Drive/\" to match your drive folder. Enter name exactly as is including spaces between double quotations. (ie \"drive/My Drive/GESDataCleaningCode\")\n",
        "\n",
        "pre_quant.to_csv('Pre_Closed.csv')\n",
        "!cp Pre_Closed.csv \"drive/My Drive/GESDataCleaningCode\"\n",
        "\n",
        "post_quant.to_csv('Post_Closed.csv')\n",
        "!cp Post_Closed.csv \"drive/My Drive/GESDataCleaningCode\"\n",
        "\n",
        "df_qual_final.to_csv('Qualitative.csv')\n",
        "!cp Qualitative.csv \"drive/My Drive/GESDataCleaningCode\"\n",
        "\n",
        "researcher.to_csv('Researcher.csv')\n",
        "!cp Researcher.csv \"drive/My Drive/GESDataCleaningCode\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}