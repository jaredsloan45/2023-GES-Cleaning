{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaredsloan45/2023-GES-Cleaning/blob/main/GES_cleaning_user_2022_2023_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oQUKPaWTqJFJ",
      "metadata": {
        "id": "oQUKPaWTqJFJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4nI7fWmqJCB",
      "metadata": {
        "id": "a4nI7fWmqJCB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "XzrNvR4LO5e0",
      "metadata": {
        "id": "XzrNvR4LO5e0"
      },
      "source": [
        "# **Survey Data Cleaning**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ofdp6boM2AVK",
      "metadata": {
        "id": "ofdp6boM2AVK"
      },
      "source": [
        "**To start:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Download chrome extension or chrome ap: Open in Colab (program that allows you to write and execute Python in your browser it has nothing to do with the CBGLCollab). Only need to do this once.\n",
        "\n",
        "Download data from qualtrics: File must be in csv not excel and \"use numeric values.\" Don't modify or delete any rows.\n",
        "\n",
        "If necessary, re-code any programs in qualtrics to match code book.\n",
        "\n",
        "Press the first \"play\" button below. Allow colab to access your drive when prompted and then select \"choose files.\" Open the desired file.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3823b79a-bd81-425f-9328-07c976eedd08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "3823b79a-bd81-425f-9328-07c976eedd08",
        "outputId": "b4f499e6-22a1-4829-8007-e32815583673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f9321ca-ab7c-4cf3-920f-9a052b43e629\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f9321ca-ab7c-4cf3-920f-9a052b43e629\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving GES_Fall22_Summer23_Oct11_23_Export.csv to GES_Fall22_Summer23_Oct11_23_Export (2).csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import natsort as ns\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import io\n",
        "drive.mount('drive',force_remount=True)\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3a767e-057b-4b89-a7dc-c2d981ecf007",
      "metadata": {
        "id": "bd3a767e-057b-4b89-a7dc-c2d981ecf007"
      },
      "source": [
        "5.   In the box below enter the:*actions for 22-23 cycle\n",
        "\n",
        "*a) file name exactly as written (including spaces) btw single quotations w .csv at the end (ie: '2021-2022 GES.csv')\n",
        "\n",
        "b) Redundant_program_cols (this is for insitutions that have 2 questions for program code ie term number and then the program. For 22-23 cycle this is CFHI and WPI). If need to add in the future, add name of the column that you want to delete). Don't need to do anything for 22-23 cycle.\n",
        "\n",
        "c) multi_answer_qs (name of the column as qualtrics geneates it, number of possible answer choices). If needed can adjust in the future. Don't need to do anything for 22-23 cycle.  \n",
        "\n",
        "*d) starting unique id number into the appropriate variables below.\n",
        "\n",
        "e) open_questions_pre/post: the name of the columns with any qualitative questions (e.g. 'Q14'). To update, either delete old column names or add new ones in quotes, separating each column with a comma. This includes any qualitative institution-specific questions. Don't need to do anything for the 22-23 cycle.\n",
        "\n",
        "f) useful_metadata: the name of any columns containing metadata that we want to include in the final files (e.g. name, intitution, program, etc.). The formatting for this is the same as the open_questions_pre/post. Don't need to do anything for the 22-23 cycle.\n",
        "*6.   Scroll to the bottom of the code (#export files to drive) and enter the google drive folder where you want your cleaned data to go.\n",
        "7.   Check the \"GES data cleaning code documentation\" to see if anything else needs to be updated. Don't need to do anything for the 22-23 cycle.\n",
        "*8.   Press the second \"play\" button to run the rest of the code, the cleaned files should appear in the desired folder after about a minute\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import prepare_class\n",
        "\n",
        "#1:User inputted variables\n",
        "file_name = 'GES_Fall22_Summer23_Oct11_23_Export (2).csv'\n",
        "redundant_program_cols = ['CFHI','WPI']\n",
        "multi_answer_qs = [ ['PF42a',7],['PF43',10]]\n",
        "id_start = 12200\n",
        "open_questions_pre = ['Q14', 'Q15', 'Q17', 'Q18', 'Q27', 'Q36', 'Q45', 'Q46', 'Q54', 'Q55', 'Q64', 'Q71', 'Q82', 'Q84_4_TEXT', 'Q85_8_TEXT', 'Q86_2_TEXT', 'Q87_2_TEXT', 'Q98_11_12_TEXT', 'ElWAD1', 'ElWAD2', 'ELWAD3', 'ElWAD4', 'ElWAD5', 'ElH1', 'ElH2', 'ElH3', 'WPI1a']\n",
        "open_questions_post = [ 'Q14P', 'Q15P', 'Q17P', 'Q18P', 'Q27P', 'Q36P', 'Q45P', 'Q46P', 'Q54P', 'Q55P', 'Q64P', 'Q71P', 'Q82P', 'Q137P', 'Q139P', 'Q141P', 'Q143P', 'Q149P', 'Q151P', 'Q153P', 'PF33_10_TEXT', 'PF34_3_TEXT', 'PF35_6_TEXT', 'PF36_7_TEXT', 'PF41_1_TEXT','ELWAD1P', 'ElWAD2P', 'ELWAD3P', 'ELWAD4P', 'ElH1P', 'ElH2P', 'ElH3P', 'ElH4P', 'ElH5P', 'WPI1aP', 'WPI2P', 'WPI3P', 'WPI4P', 'Have2P']\n",
        "useful_metadata = ['Q1_1','Q1_2','Q1_3','UniqueID', 'Institution','Program','Pre_Post_x','Pre_Post_y']\n",
        "#read file into dataframe\n",
        "df1 = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "question_text = df1.iloc[:1,:]\n",
        "question_text = question_text[open_questions_pre + open_questions_post]\n",
        "df1 = df1.iloc[2:,:]\n",
        "\n",
        "#2:Initial formatting changes:\n",
        "\n",
        "#fixing the institution column name - Qualtrics generates it with extra characters\n",
        "df1 = df1.rename(\n",
        "    columns={\n",
        "        'Institution\\xa0':'Institution',\n",
        "      })\n",
        "\n",
        "#Combine all program columns into one\n",
        "#first delete redundant columns (e.g. the place-based/virtual/other question for CFHI)\n",
        "df1 = df1.drop(df1.loc[:,redundant_program_cols],axis = 1)\n",
        "\n",
        "#Then combine remaining columns -- take all cols betweeen \"institution\" and \"email\" and merge them, each row should only have a value in one column\n",
        "program_start = df1.columns.get_loc('Institution')+1\n",
        "program_end = df1.columns.get_loc('Email')\n",
        "df1 = df1.rename(\n",
        "    columns = {\n",
        "        df1.columns[program_start]: \"Program\",\n",
        "    }\n",
        ")\n",
        "for i in range(program_start,program_end):\n",
        "  if df1.columns[i] not in redundant_program_cols:\n",
        "    df1['Program'] = df1['Program'].combine_first(df1.iloc[:,i])\n",
        "df1 = df1.drop(df1.iloc[:,program_start+1:program_end],axis=1)\n",
        "\n",
        "#Formatting emails and middle initial/name -- make emails lowercase and trim whitespace to serve as a unique identifier, removes middle initials entered as \"None\" or \"N/A\"\n",
        "df1['Email'] = df1['Email'].str.lower()\n",
        "df1['Email'] = df1['Email'].str.strip()\n",
        "df1['Q1_2'] = df1['Q1_2'].str.title().str.strip()\n",
        "df1.loc[(df1['Q1_2']=='None')|(df1['Q1_2']=='N/A'),'Q1_2'] = pd.Series(dtype=str)\n",
        "\n",
        "#remove rows without values in Q6\n",
        "df1 = df1[pd.notna(df1['Q6'])|pd.notna(df1['Q6P'])]\n",
        "print(df1[pd.isna(df1['Program'])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDeyDmJMcmY",
        "outputId": "abbb9465-2b13-461b-f914-59ceb1abab5c"
      },
      "id": "NdDeyDmJMcmY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                StartDate              EndDate Status        IPAddress  \\\n",
            "2645  2023-06-06 14:43:28  2023-06-06 14:45:49      0  129.170.194.147   \n",
            "\n",
            "     Progress Duration (in seconds) Finished         RecordedDate  \\\n",
            "2645      100                   141        1  2023-06-06 14:45:50   \n",
            "\n",
            "             ResponseId RecipientLastName  ...   CRP   CR GCRP  GCR HRBP  HRB  \\\n",
            "2645  R_2e2Ol04TtlnofEJ               NaN  ...  3.25  NaN  4.0  NaN  2.5  NaN   \n",
            "\n",
            "        ODP   OD  PVP   PV  \n",
            "2645  3.125  NaN  3.5  NaN  \n",
            "\n",
            "[1 rows x 268 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-9a4f336068e2>:12: DtypeWarning: Columns (2,4,5,6,9,10,11,12,13,14,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,42,43,44,45,46,47,48,49,50,53,56,57,58,59,60,61,62,63,65,66,67,68,69,70,71,72,74,75,76,77,78,79,82,83,84,85,88,89,90,91,92,93,94,96,97,98,99,100,101,103,104,105,106,107,108,109,110,111,112,114,115,119,121,123,124,125,126,127,128,129,130,131,132,133,134,135,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,159,162,163,164,165,166,167,168,169,171,172,173,174,175,176,177,178,180,181,182,183,184,185,188,189,190,191,194,195,196,197,198,199,200,202,203,204,205,206,207,209,210,211,212,213,214,215,216,217,218,220,222,229,231,233,235,237,239,240,241,242,244,247,248,249,250,251,252,253,254,259,260,261,262,263,264,269,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df1 = pd.read_csv(io.BytesIO(uploaded[file_name]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bef7b9b-bfbc-4284-a48e-05b483e10b0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "2bef7b9b-bfbc-4284-a48e-05b483e10b0d",
        "outputId": "2f3eca80-6e55-43ab-9692-8e7ad8bc5e0a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d22e36b50735>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0museful_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Q1_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q1_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q1_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'UniqueID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Institution'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Program'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Pre_Post_x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Pre_Post_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#read file into dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mquestion_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'GES_Fall22_Summer23_Oct11_23_Export.csv'"
          ]
        }
      ],
      "source": [
        "from types import prepare_class\n",
        "\n",
        "#1:User inputted variables\n",
        "file_name = 'GES_Fall22_Summer23_Oct11_23_Export (1).csv'\n",
        "redundant_program_cols = ['CFHI','WPI']\n",
        "multi_answer_qs = [ ['PF42a',7],['PF43',10]]\n",
        "id_start = 12200\n",
        "open_questions_pre = ['Q14', 'Q15', 'Q17', 'Q18', 'Q27', 'Q36', 'Q45', 'Q46', 'Q54', 'Q55', 'Q64', 'Q71', 'Q82', 'Q84_4_TEXT', 'Q85_8_TEXT', 'Q86_2_TEXT', 'Q87_2_TEXT', 'Q98_11_12_TEXT', 'ElWAD1', 'ElWAD2', 'ELWAD3', 'ElWAD4', 'ElWAD5', 'ElH1', 'ElH2', 'ElH3', 'WPI1a']\n",
        "open_questions_post = [ 'Q14P', 'Q15P', 'Q17P', 'Q18P', 'Q27P', 'Q36P', 'Q45P', 'Q46P', 'Q54P', 'Q55P', 'Q64P', 'Q71P', 'Q82P', 'Q137P', 'Q139P', 'Q141P', 'Q143P', 'Q149P', 'Q151P', 'Q153P', 'PF33_10_TEXT', 'PF34_3_TEXT', 'PF35_6_TEXT', 'PF36_7_TEXT', 'PF41_1_TEXT','ELWAD1P', 'ElWAD2P', 'ELWAD3P', 'ELWAD4P', 'ElH1P', 'ElH2P', 'ElH3P', 'ElH4P', 'ElH5P', 'WPI1aP', 'WPI2P', 'WPI3P', 'WPI4P', 'Have2P']\n",
        "useful_metadata = ['Q1_1','Q1_2','Q1_3','UniqueID', 'Institution','Program','Pre_Post_x','Pre_Post_y']\n",
        "#read file into dataframe\n",
        "df1 = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "question_text = df1.iloc[:1,:]\n",
        "question_text = question_text[open_questions_pre + open_questions_post]\n",
        "df1 = df1.iloc[2:,:]\n",
        "\n",
        "#2:Initial formatting changes:\n",
        "\n",
        "#fixing the institution column name - Qualtrics generates it with extra characters\n",
        "df1 = df1.rename(\n",
        "    columns={\n",
        "        'Institution\\xa0':'Institution',\n",
        "      })\n",
        "\n",
        "#Combine all program columns into one\n",
        "#first delete redundant columns (e.g. the place-based/virtual/other question for CFHI)\n",
        "df1 = df1.drop(df1.loc[:,redundant_program_cols],axis = 1)\n",
        "\n",
        "#Then combine remaining columns -- take all cols betweeen \"institution\" and \"email\" and merge them, each row should only have a value in one column\n",
        "program_start = df1.columns.get_loc('Institution')+1\n",
        "program_end = df1.columns.get_loc('Email')\n",
        "df1 = df1.rename(\n",
        "    columns = {\n",
        "        df1.columns[program_start]: \"Program\",\n",
        "    }\n",
        ")\n",
        "for i in range(program_start,program_end):\n",
        "  if df1.columns[i] not in redundant_program_cols:\n",
        "    df1['Program'] = df1['Program'].combine_first(df1.iloc[:,i])\n",
        "df1 = df1.drop(df1.iloc[:,program_start+1:program_end],axis=1)\n",
        "\n",
        "#Formatting emails and middle initial/name -- make emails lowercase and trim whitespace to serve as a unique identifier, removes middle initials entered as \"None\" or \"N/A\"\n",
        "df1['Email'] = df1['Email'].str.lower()\n",
        "df1['Email'] = df1['Email'].str.strip()\n",
        "df1['Q1_2'] = df1['Q1_2'].str.title().str.strip()\n",
        "df1.loc[(df1['Q1_2']=='None')|(df1['Q1_2']=='N/A'),'Q1_2'] = pd.Series(dtype=str)\n",
        "\n",
        "#remove rows without values in Q6\n",
        "df1 = df1[pd.notna(df1['Q6'])|pd.notna(df1['Q6P'])]\n",
        "\n",
        "#3:Drop duplicates\n",
        "\n",
        "#sort by progress and duration\n",
        "df = df1.sort_values(by=['Progress','Duration (in seconds)'],axis=0,ascending=[False,False])\n",
        "#delete unneeded metadata columns\n",
        "df = df.drop(['Status','IPAddress'],axis=1).drop(df.loc[:,'Duration (in seconds)':'UserLanguage'],axis=1)\n",
        "#drop duplicates -- because of the sorting, we are able to simply keep the first record of any duplicates\n",
        "df = df.drop_duplicates(subset=['Email','Program','Pre_Post'],keep='first')\n",
        "\n",
        "df = df[pd.notna(df['Program']) & pd.notna(df['Pre_Post'])]\n",
        "\n",
        "#ensure pre_post column stored as numbers rather than text\n",
        "df['Pre_Post']=df['Pre_Post'].astype(np.int32)\n",
        "df['Program']=df['Program'].astype(np.int32)\n",
        "#4:Unique IDs\n",
        "#create UniqueID column\n",
        "df.insert(df.columns.get_loc('Q1_3')+1,'UniqueID',\"\")\n",
        "\n",
        "#Generate unique ids for pre\n",
        "unique_ids = range(id_start,id_start+len(df[df['Pre_Post']<2]))\n",
        "df.loc[df['Pre_Post']<2,'UniqueID']=unique_ids\n",
        "\n",
        "#Generate unique ids for post\n",
        "\n",
        "#create subset of the pre data containing just emails, program # and UniqueIDs\n",
        "emails_pre = df.loc[df['Pre_Post']<2,['Email','Program','UniqueID']]\n",
        "emails_pre = emails_pre.rename(\n",
        "    columns ={\n",
        "        'UniqueID':'id'\n",
        "    })\n",
        "\n",
        "#split pre and post data\n",
        "df_pre = df[df['Pre_Post']<2]\n",
        "df_post1 = df[df['Pre_Post']>1]\n",
        "\n",
        "#Assigns unique ids for post-survey matched cases\n",
        "df_post = pd.merge(df_post1, emails_pre, on =['Email','Program'], how ='left')\n",
        "\n",
        "df_post['UniqueID']=df_post['id']\n",
        "#removes redundant column\n",
        "df_post = df_post.drop('id',axis=1)\n",
        "#generate ids for unmatched post starting at 1+the max of the unique ids in the pre-survey\n",
        "post_ids = range(int(max(df_pre['UniqueID']))+1,len(df_post[pd.isna(df_post['UniqueID'])])+int(max(df_pre['UniqueID']))+1)\n",
        "df_post.loc[pd.isna(df_post['UniqueID']),'UniqueID']=post_ids\n",
        "\n",
        "#5:Begin building output files\n",
        "\n",
        "#create researcher file\n",
        "researcher = df_post[['UniqueID','Q1_1','Q1_2','Q1_3', 'Institution', 'Program', 'Res1','Res2']]\n",
        "\n",
        "#remove post questions from pre-survey and pre questions from post-survey\n",
        "df_pre_cleaned = df_pre.drop(df_pre.loc[:,'Q6P':],axis=1,inplace=False)\n",
        "index1 = df_post.columns.get_loc('Q6P')\n",
        "index2 = df_post.columns.get_loc('Q6')\n",
        "df_post_cleaned = df_post.drop(df_post.iloc[:,index2:index1],axis=1,inplace=False).drop(df_post.loc[:,['Res1','Res2']],axis=1,inplace=False).drop(df_post.loc[:,'CAP':],axis=1,inplace=False)\n",
        "\n",
        "#6:Adjust demographic and program factor data\n",
        "\n",
        "#For any students selecting multiple racial categories, re-assigns column to \"8\" for \"other\"\n",
        "df_pre_cleaned.loc[df_pre_cleaned['Q85'].str.len()>1,'Q85']=8\n",
        "#renames Q95_1 and Q95_2 into Q95 and Q96 respectively\n",
        "df_pre_cleaned.rename(\n",
        "    columns = {\n",
        "        'Q95_1':'Q95',\n",
        "        'Q95_2':'Q96'\n",
        "    },inplace=True)\n",
        "\n",
        "\n",
        "#Split any columns with multiple possible answers into separate columns for each choice -- 1 for yes, empty for no\n",
        "for array in multi_answer_qs:\n",
        "  cols_add = []\n",
        "  col_loc = df_post_cleaned.columns.get_loc(array[0])\n",
        "  for i in range(array[1],0,-1):\n",
        "    col_name = array[0] + '_' + str(i)\n",
        "    df_post_cleaned.insert(col_loc,col_name,\"\")\n",
        "    cols_add.append([i,col_name])\n",
        "    df_post_cleaned.loc[df_post_cleaned[array[0]].str.find(str(i))>-1,col_name]=1\n",
        "  df_post_cleaned = df_post_cleaned.drop(array[0],axis=1)\n",
        "\n",
        "#7:Generate final outputs\n",
        "\n",
        "# Split into quant and qual based on user-inputted lists at top\n",
        "pre_open_q = []\n",
        "q_names_pre = []\n",
        "for array in open_questions_pre:\n",
        "  pre_open_q.append(array[0])\n",
        "  q_names_pre.append(array[1])\n",
        "post_open_q = []\n",
        "q_names_post = []\n",
        "for array in open_questions_post:\n",
        "  post_open_q.append(array[0])\n",
        "  q_names_post.append(array[1])\n",
        "\n",
        "pre_quant = df_pre_cleaned.drop(open_questions_pre,axis=1)\n",
        "post_quant = df_post_cleaned.drop(open_questions_post,axis=1)\n",
        "\n",
        "\n",
        "#Create matched column for quant -- for the pre data, check each case to see if it exists in the post, assign value to \"Matched\" column accordingly, same for post.\n",
        "quant_dbs = [pre_quant,post_quant]\n",
        "for i in range(2):\n",
        "  quant_dbs[i].insert(quant_dbs[i].columns.get_loc('Program')+1,'Matched',\"\")\n",
        "  quant_dbs[i].loc[quant_dbs[i]['UniqueID'].isin(quant_dbs[(i+1)%2]['UniqueID']),'Matched'] =3\n",
        "  quant_dbs[i].loc[~(quant_dbs[i]['UniqueID'].isin(quant_dbs[(i+1)%2]['UniqueID'])),'Matched'] = i+1\n",
        "\n",
        "#9:Merging qual responses\n",
        "\n",
        "#recreate the full dataset (qual and quant, pre and post)\n",
        "df_full = pd.merge(df_pre_cleaned.loc[:,'Q1_1':],df_post_cleaned.loc[:,'UniqueID':],on=['UniqueID','Institution','Program'],how='outer')\n",
        "\n",
        "#Add the qual question text into the dataset\n",
        "df_qual_combined = pd.concat([question_text, df_full], ignore_index=True)\n",
        "#Filter full dataset to include only qual questions and the necessary metadata\n",
        "df_qual = df_qual_combined[useful_metadata + open_questions_pre + open_questions_post]\n",
        "\n",
        "#reorder qual columns\n",
        "\n",
        "#The idea here is to search the column names for certain letters that are indicative of different questions -- we can identify the program factors because they start with \"PF\", the demographic questions because they start with \"Q\" and end with \"TEXT\", and the institution specific questions as the only non-metadata columns that don't start with \"Q\" or \"PF\".\n",
        "qual_cols = df_qual.columns[8:]\n",
        "qual_qs=[]\n",
        "demog_qs = []\n",
        "qual_program_factors = []\n",
        "qual_info = []\n",
        "inst_spec_qual = []\n",
        "#need to make sure _TEXT ending is specific to demographic qs (not including program factors or institution-specific qs)\n",
        "for col in qual_cols:\n",
        "  if re.search('^Q',col) != None:\n",
        "    if re.search('TEXT$',col) != None:\n",
        "      demog_qs.append(col)\n",
        "    else:\n",
        "      qual_qs.append(col)\n",
        "  elif re.search('^PF',col) != None:\n",
        "    qual_program_factors.append(col)\n",
        "  else:\n",
        "    inst_spec_qual.append(col)\n",
        "#Take the separate sets of column names and sort them to put, for example, \"Q14P\" directly after \"Q14\". The ns.natsorted function accomplishes this, since a standard sort would place \"Q120\" before \"Q15\".\n",
        "sorted_qs = ns.natsorted(qual_qs)\n",
        "sorted_pf = ns.natsorted(qual_program_factors)\n",
        "sorted_is = ns.natsorted(inst_spec_qual,alg=ns.IGNORECASE)\n",
        "sorted_ds = ns.natsorted(demog_qs)\n",
        "\n",
        "#Generate a new Pre_Post column to reflect the \"matched\" quant column (1 for pre only, 2 for post only, 3 for both)\n",
        "df_qual['Pre_Post']=df_qual['Pre_Post_x'].fillna(0)+df_qual['Pre_Post_y'].fillna(0)\n",
        "df_qual_final = df_qual[['Q1_1','Q1_2','Q1_3','UniqueID','Institution','Program','Pre_Post'] +  sorted_qs + sorted_ds + sorted_pf + sorted_is]\n",
        "\n",
        "#export files to drive -- edit text after \"drive/My Drive/\" to match your drive folder. Enter name exactly as is including spaces between double quotations. (ie \"drive/My Drive/GESDataCleaningCode\")\n",
        "\n",
        "pre_quant.to_csv('Pre_Closed.csv')\n",
        "!cp Pre_Closed.csv \"drive/My Drive/GES Cleaned\"\n",
        "\n",
        "post_quant.to_csv('Post_Closed.csv')\n",
        "!cp Post_Closed.csv \"drive/My Drive/GES Cleaned\"\n",
        "\n",
        "df_qual_final.to_csv('Qualitative.csv')\n",
        "!cp Qualitative.csv \"drive/My Drive/GES Cleaned\"\n",
        "\n",
        "researcher.to_csv('Researcher.csv')\n",
        "!cp Researcher.csv \"drive/My Drive/GES Cleaned\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}